# Redes_neurais

Este reposit√≥rio demonstra implementa√ß√µes b√°sicas de **Perceptrons** e **Adaline** em JavaScript e Python, e inclui exemplos de aplica√ß√£o em diversos conjuntos de dados (flores, diabetes). O objetivo √© ilustrar o funcionamento interno desses algoritmos de aprendizagem supervisionada passo a passo.

---

## üìñ Descri√ß√£o dos Algoritmos

---

### Perceptron

- **Tipo**: Classificador linear bin√°rio.  
- **Como funciona**: ajusta pesos `w` e bias `b` para minimizar erros de classifica√ß√£o. A cada itera√ß√£o, para cada amostra **x**, calcula:
  
  $$y_{\text{pred}} = \mathrm{sign}(w \cdot x + b)$$

  e atualiza:

  $$
  \begin{aligned}
    w &\leftarrow w + \eta\,(y_{\text{true}} - y_{\text{pred}})\,x,\\
    b &\leftarrow b + \eta\,(y_{\text{true}} - y_{\text{pred}})
  \end{aligned}
  $$

  onde `Œ∑` √© a taxa de aprendizado.

- **Implementa√ß√£o** (`perceptron.js`):
  1. Classe `Perceptron` com m√©todos `train()` e `predict()`.  
  2. Exemplo de uso no final do arquivo.

---

### Adaline (Adaptive Linear Neuron)

- **Tipo**: Rede de camada √∫nica com sa√≠da cont√≠nua e fun√ß√£o de ativa√ß√£o linear.
- **Como funciona**: minimiza o erro quadr√°tico m√©dio (MSE)
  
  $$
  J(w,b) = \frac{1}{2N} \sum_{i=1}^N \bigl(w \cdot x^{(i)} + b - y^{(i)}\bigr)^2
  $$

  usando **gradiente descendente**. As atualiza√ß√µes s√£o:

  $$
  \begin{aligned}
    w &\leftarrow w - \eta \,\frac{\partial J}{\partial w},\\
    b &\leftarrow b - \eta \,\frac{\partial J}{\partial b}.
  \end{aligned}
  $$

- **Implementa√ß√£o** (`adalaine.py`):
  1. Classe `Adaline` com m√©todos:
     - `fit()`  
     - `net_input()`  
     - `activation()`  
     - `predict()`  
  2. Registro de hist√≥rico de custo por √©poca para an√°lise de converg√™ncia.

- **Script de exemplo** (`flower_adalaine.py`):
  1. Carrega `dataFlower.csv` (ex.: Iris simplificado).  
  2. Normaliza caracter√≠sticas.  
  3. Treina Adaline e plota a evolu√ß√£o do custo ao longo das √©pocas.

---
